{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation=nn.Identity()):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AskPrice1</th>\n",
       "      <th>BidPrice1</th>\n",
       "      <th>AskVolume1</th>\n",
       "      <th>BidVolume1</th>\n",
       "      <th>AskPrice5</th>\n",
       "      <th>AskPrice4</th>\n",
       "      <th>AskPrice3</th>\n",
       "      <th>AskPrice2</th>\n",
       "      <th>BidPrice2</th>\n",
       "      <th>BidPrice3</th>\n",
       "      <th>...</th>\n",
       "      <th>BidPrice5</th>\n",
       "      <th>AskVolume5</th>\n",
       "      <th>AskVolume4</th>\n",
       "      <th>AskVolume3</th>\n",
       "      <th>AskVolume2</th>\n",
       "      <th>BidVolume2</th>\n",
       "      <th>BidVolume3</th>\n",
       "      <th>BidVolume4</th>\n",
       "      <th>BidVolume5</th>\n",
       "      <th>trading_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>705.5</td>\n",
       "      <td>705.0</td>\n",
       "      <td>82</td>\n",
       "      <td>304</td>\n",
       "      <td>707.5</td>\n",
       "      <td>707.0</td>\n",
       "      <td>706.5</td>\n",
       "      <td>706.0</td>\n",
       "      <td>704.5</td>\n",
       "      <td>704.0</td>\n",
       "      <td>...</td>\n",
       "      <td>703.0</td>\n",
       "      <td>25</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>53</td>\n",
       "      <td>56</td>\n",
       "      <td>95</td>\n",
       "      <td>22</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>707.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>709.5</td>\n",
       "      <td>709.0</td>\n",
       "      <td>708.5</td>\n",
       "      <td>708.0</td>\n",
       "      <td>705.5</td>\n",
       "      <td>705.0</td>\n",
       "      <td>...</td>\n",
       "      <td>704.0</td>\n",
       "      <td>24</td>\n",
       "      <td>124</td>\n",
       "      <td>4</td>\n",
       "      <td>272</td>\n",
       "      <td>376</td>\n",
       "      <td>300</td>\n",
       "      <td>56</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>707.0</td>\n",
       "      <td>706.5</td>\n",
       "      <td>36</td>\n",
       "      <td>29</td>\n",
       "      <td>709.0</td>\n",
       "      <td>708.5</td>\n",
       "      <td>708.0</td>\n",
       "      <td>707.5</td>\n",
       "      <td>706.0</td>\n",
       "      <td>705.5</td>\n",
       "      <td>...</td>\n",
       "      <td>704.5</td>\n",
       "      <td>133</td>\n",
       "      <td>46</td>\n",
       "      <td>284</td>\n",
       "      <td>97</td>\n",
       "      <td>4</td>\n",
       "      <td>440</td>\n",
       "      <td>396</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>706.5</td>\n",
       "      <td>705.5</td>\n",
       "      <td>16</td>\n",
       "      <td>74</td>\n",
       "      <td>708.5</td>\n",
       "      <td>708.0</td>\n",
       "      <td>707.5</td>\n",
       "      <td>707.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>704.5</td>\n",
       "      <td>...</td>\n",
       "      <td>703.5</td>\n",
       "      <td>47</td>\n",
       "      <td>302</td>\n",
       "      <td>95</td>\n",
       "      <td>40</td>\n",
       "      <td>409</td>\n",
       "      <td>80</td>\n",
       "      <td>125</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>706.5</td>\n",
       "      <td>705.5</td>\n",
       "      <td>35</td>\n",
       "      <td>116</td>\n",
       "      <td>708.5</td>\n",
       "      <td>708.0</td>\n",
       "      <td>707.5</td>\n",
       "      <td>707.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>704.5</td>\n",
       "      <td>...</td>\n",
       "      <td>703.5</td>\n",
       "      <td>58</td>\n",
       "      <td>296</td>\n",
       "      <td>95</td>\n",
       "      <td>80</td>\n",
       "      <td>414</td>\n",
       "      <td>80</td>\n",
       "      <td>128</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AskPrice1  BidPrice1  AskVolume1  BidVolume1  AskPrice5  AskPrice4  \\\n",
       "0      705.5      705.0          82         304      707.5      707.0   \n",
       "1      707.0      706.0           7          37      709.5      709.0   \n",
       "2      707.0      706.5          36          29      709.0      708.5   \n",
       "3      706.5      705.5          16          74      708.5      708.0   \n",
       "4      706.5      705.5          35         116      708.5      708.0   \n",
       "\n",
       "   AskPrice3  AskPrice2  BidPrice2  BidPrice3  ...  BidPrice5  AskVolume5  \\\n",
       "0      706.5      706.0      704.5      704.0  ...      703.0          25   \n",
       "1      708.5      708.0      705.5      705.0  ...      704.0          24   \n",
       "2      708.0      707.5      706.0      705.5  ...      704.5         133   \n",
       "3      707.5      707.0      705.0      704.5  ...      703.5          47   \n",
       "4      707.5      707.0      705.0      704.5  ...      703.5          58   \n",
       "\n",
       "   AskVolume4  AskVolume3  AskVolume2  BidVolume2  BidVolume3  BidVolume4  \\\n",
       "0          68          16          53          56          95          22   \n",
       "1         124           4         272         376         300          56   \n",
       "2          46         284          97           4         440         396   \n",
       "3         302          95          40         409          80         125   \n",
       "4         296          95          80         414          80         128   \n",
       "\n",
       "   BidVolume5  trading_day  \n",
       "0          56            1  \n",
       "1          95            1  \n",
       "2         124            1  \n",
       "3          31            1  \n",
       "4          39            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AskPrice1</th>\n",
       "      <th>BidPrice1</th>\n",
       "      <th>AskVolume1</th>\n",
       "      <th>BidVolume1</th>\n",
       "      <th>AskPrice5</th>\n",
       "      <th>AskPrice4</th>\n",
       "      <th>AskPrice3</th>\n",
       "      <th>AskPrice2</th>\n",
       "      <th>BidPrice2</th>\n",
       "      <th>BidPrice3</th>\n",
       "      <th>BidPrice4</th>\n",
       "      <th>BidPrice5</th>\n",
       "      <th>AskVolume5</th>\n",
       "      <th>AskVolume4</th>\n",
       "      <th>AskVolume3</th>\n",
       "      <th>AskVolume2</th>\n",
       "      <th>BidVolume2</th>\n",
       "      <th>BidVolume3</th>\n",
       "      <th>BidVolume4</th>\n",
       "      <th>BidVolume5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>686982</th>\n",
       "      <td>727.0</td>\n",
       "      <td>726.5</td>\n",
       "      <td>59</td>\n",
       "      <td>527</td>\n",
       "      <td>729.0</td>\n",
       "      <td>728.5</td>\n",
       "      <td>728.0</td>\n",
       "      <td>727.5</td>\n",
       "      <td>726.0</td>\n",
       "      <td>725.5</td>\n",
       "      <td>725.0</td>\n",
       "      <td>724.5</td>\n",
       "      <td>1147</td>\n",
       "      <td>734</td>\n",
       "      <td>1458</td>\n",
       "      <td>738</td>\n",
       "      <td>811</td>\n",
       "      <td>436</td>\n",
       "      <td>765</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686983</th>\n",
       "      <td>727.0</td>\n",
       "      <td>726.5</td>\n",
       "      <td>78</td>\n",
       "      <td>528</td>\n",
       "      <td>729.0</td>\n",
       "      <td>728.5</td>\n",
       "      <td>728.0</td>\n",
       "      <td>727.5</td>\n",
       "      <td>726.0</td>\n",
       "      <td>725.5</td>\n",
       "      <td>725.0</td>\n",
       "      <td>724.5</td>\n",
       "      <td>1147</td>\n",
       "      <td>734</td>\n",
       "      <td>1458</td>\n",
       "      <td>735</td>\n",
       "      <td>811</td>\n",
       "      <td>436</td>\n",
       "      <td>765</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686984</th>\n",
       "      <td>727.0</td>\n",
       "      <td>726.5</td>\n",
       "      <td>84</td>\n",
       "      <td>528</td>\n",
       "      <td>729.0</td>\n",
       "      <td>728.5</td>\n",
       "      <td>728.0</td>\n",
       "      <td>727.5</td>\n",
       "      <td>726.0</td>\n",
       "      <td>725.5</td>\n",
       "      <td>725.0</td>\n",
       "      <td>724.5</td>\n",
       "      <td>1147</td>\n",
       "      <td>734</td>\n",
       "      <td>1458</td>\n",
       "      <td>735</td>\n",
       "      <td>811</td>\n",
       "      <td>436</td>\n",
       "      <td>765</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686985</th>\n",
       "      <td>727.0</td>\n",
       "      <td>726.5</td>\n",
       "      <td>85</td>\n",
       "      <td>523</td>\n",
       "      <td>729.0</td>\n",
       "      <td>728.5</td>\n",
       "      <td>728.0</td>\n",
       "      <td>727.5</td>\n",
       "      <td>726.0</td>\n",
       "      <td>725.5</td>\n",
       "      <td>725.0</td>\n",
       "      <td>724.5</td>\n",
       "      <td>1148</td>\n",
       "      <td>734</td>\n",
       "      <td>1458</td>\n",
       "      <td>736</td>\n",
       "      <td>811</td>\n",
       "      <td>436</td>\n",
       "      <td>765</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686986</th>\n",
       "      <td>727.0</td>\n",
       "      <td>726.5</td>\n",
       "      <td>112</td>\n",
       "      <td>528</td>\n",
       "      <td>729.0</td>\n",
       "      <td>728.5</td>\n",
       "      <td>728.0</td>\n",
       "      <td>727.5</td>\n",
       "      <td>726.0</td>\n",
       "      <td>725.5</td>\n",
       "      <td>725.0</td>\n",
       "      <td>724.5</td>\n",
       "      <td>1148</td>\n",
       "      <td>734</td>\n",
       "      <td>1458</td>\n",
       "      <td>737</td>\n",
       "      <td>811</td>\n",
       "      <td>436</td>\n",
       "      <td>764</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AskPrice1  BidPrice1  AskVolume1  BidVolume1  AskPrice5  AskPrice4  \\\n",
       "686982      727.0      726.5          59         527      729.0      728.5   \n",
       "686983      727.0      726.5          78         528      729.0      728.5   \n",
       "686984      727.0      726.5          84         528      729.0      728.5   \n",
       "686985      727.0      726.5          85         523      729.0      728.5   \n",
       "686986      727.0      726.5         112         528      729.0      728.5   \n",
       "\n",
       "        AskPrice3  AskPrice2  BidPrice2  BidPrice3  BidPrice4  BidPrice5  \\\n",
       "686982      728.0      727.5      726.0      725.5      725.0      724.5   \n",
       "686983      728.0      727.5      726.0      725.5      725.0      724.5   \n",
       "686984      728.0      727.5      726.0      725.5      725.0      724.5   \n",
       "686985      728.0      727.5      726.0      725.5      725.0      724.5   \n",
       "686986      728.0      727.5      726.0      725.5      725.0      724.5   \n",
       "\n",
       "        AskVolume5  AskVolume4  AskVolume3  AskVolume2  BidVolume2  \\\n",
       "686982        1147         734        1458         738         811   \n",
       "686983        1147         734        1458         735         811   \n",
       "686984        1147         734        1458         735         811   \n",
       "686985        1148         734        1458         736         811   \n",
       "686986        1148         734        1458         737         811   \n",
       "\n",
       "        BidVolume3  BidVolume4  BidVolume5  \n",
       "686982         436         765         372  \n",
       "686983         436         765         372  \n",
       "686984         436         765         372  \n",
       "686985         436         765         372  \n",
       "686986         436         764         372  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = df[df['trading_day'] == 10]\n",
    "test_data = df[df['trading_day'] == 13]\n",
    "train_data = train_data.drop(['trading_day'], axis=1)\n",
    "test_data = test_data.drop(['trading_day'], axis=1)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to get a path from the policy network\n",
    "def get_path(policy_network, train_dataset):\n",
    "    holding_positions = [0]\n",
    "    rewards = []\n",
    "    states = [] \n",
    "    action_dist_set = []\n",
    "    action_set = []\n",
    "    for i in range(len(train_dataset) - 1):\n",
    "\n",
    "        # bid_ask contain the information of bid and ask price and volume\n",
    "        bid_ask = train_dataset[i] \n",
    "        # hold is the current holding position\n",
    "        hold = holding_positions[-1]\n",
    "        # concat bid_ask and hold to get the state\n",
    "        state = torch.cat((bid_ask, torch.tensor([hold]).float().to(device)), dim = 0)\n",
    "        states.append(state)\n",
    "        # get the action from the policy network, which is a probability distribution\n",
    "        action_dist = policy_network(state)\n",
    "        action_dist_set.append(action_dist)\n",
    "        # sample an action from the probability distribution\n",
    "        action = torch.multinomial(action_dist, 1).item() - 1\n",
    "        action_set.append(action + 1)\n",
    "        # decide the position change based on the action and current holding position\n",
    "        if hold == 0:\n",
    "            holding_positions.append(action)\n",
    "        if hold == 1:\n",
    "            holding_positions.append(min(action + hold, hold))\n",
    "        if hold == -1:\n",
    "            holding_positions.append(max(action + hold, hold))\n",
    "\n",
    "        new_hold = holding_positions[-1]\n",
    "        \n",
    "        # compute the reward (cash change) \n",
    "        position_change = new_hold - hold\n",
    "        reward = 0\n",
    "        if position_change == 1:\n",
    "            reward = - state[0] \n",
    "        elif position_change == -1:\n",
    "            reward = state[1] \n",
    "\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    # at the end, the agent needs to liquid all positions\n",
    "    if holding_positions[-1] == 1:\n",
    "        rewards.append(train_dataset[-1][1])\n",
    "    elif holding_positions[-1] == -1:\n",
    "        rewards.append(-train_dataset[-1][0])\n",
    "    else:\n",
    "        rewards.append(0)\n",
    "\n",
    "    # convert the list of rewards to a tensor\n",
    "    rewards = torch.tensor(rewards).float().to(device)\n",
    "    # convert the list of states to a tensor\n",
    "    states = torch.stack(states).to(device)\n",
    "    action_dist_set = torch.stack(action_dist_set).to(device)\n",
    "    return states, rewards, action_dist_set, action_set\n",
    "\n",
    "\n",
    "        \n",
    "# train the value network using the TD(0) algorithm\n",
    "def train_value_network(value_network, policy_network, train_dataset, value_optimizer, gamma, epochs = 10, lr = 0.001):\n",
    "    # define the loss function\n",
    "    loss = 0\n",
    "    # loop over the epochs\n",
    "    for epoch in range(epochs):\n",
    "        # get the path from the policy network\n",
    "        # only use one trajectory each epoch\n",
    "        states, rewards, action_dist_set, action_set = get_path(policy_network, train_dataset)\n",
    "        # get the value estimate from the value network\n",
    "        value_estimate = value_network(states)\n",
    "        value_estimate = value_estimate.squeeze(1)\n",
    "        new_value_estimate = torch.cat((value_estimate, torch.tensor([0]).to(device)))\n",
    "        # compute the TD(0) error\n",
    "        loss = (rewards[:-1] + gamma * new_value_estimate[1:] - new_value_estimate[:-1]).pow(2).mean()\n",
    "        # zero the gradient\n",
    "        value_optimizer.zero_grad()\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        value_optimizer.step()\n",
    "        # print the loss\n",
    "        print('Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))\n",
    "    return None\n",
    "\n",
    "\n",
    "# the following define a function that compute advantage estimation for a trajectory\n",
    "def advantage_estimate(states, rewards, value_network, gamma):\n",
    "    value_estimate = value_network(states)\n",
    "    value_estimate = value_estimate.squeeze(1)\n",
    "    new_value_estimate = torch.cat((value_estimate, torch.tensor([0]).to(device)))\n",
    "    advantages = rewards[:-1] + gamma * new_value_estimate[1:] - new_value_estimate[:-1]\n",
    "    return advantages\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states, rewards, actions are trajetory data of old policy\n",
    "# there is a new_policy_network that is updated by ppo_update()\n",
    "def ppo_loss(new_policy_network, policy_network, value_network, train_dataset, batch_size, epsilon=0.2, gamma = 0.99):\n",
    "    loss = torch.tensor(0.0, requires_grad=True).to(device)\n",
    "    for _ in range(batch_size):\n",
    "        states, rewards, action_dist_set, action_set = get_path(policy_network, train_dataset)\n",
    "        new_action_dist_set = new_policy_network(states)\n",
    "        action_dist_set = action_dist_set.detach()\n",
    "        rewards = rewards.detach()\n",
    "        ratio = []\n",
    "        for i in range(len(action_dist_set)):\n",
    "            ratio.append(new_action_dist_set[i][action_set[i]] / action_dist_set[i][action_set[i]])\n",
    "        ratio = torch.stack(ratio).to(device).detach()\n",
    "        # compute the advantage of the trajectory\n",
    "        advantage = advantage_estimate(states, rewards, value_network, gamma)\n",
    "        advantage = advantage.detach()\n",
    "        # compute the clipped ratio\n",
    "        clipped_ratio = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "        # compute the surrogate loss\n",
    "        policy_loss = -torch.min(ratio * advantage, clipped_ratio * advantage).mean()\n",
    "        # compute the total loss\n",
    "        loss = loss + policy_loss\n",
    "    return loss\n",
    "\n",
    "    \n",
    "def ppo_train(new_policy_network, policy_network, value_network, optimizer, train_dataset, batch_size, epochs, epsilon=0.2, gamma = 0.99):\n",
    "    for epoch in range(epochs):\n",
    "        loss = ppo_loss(new_policy_network, policy_network, value_network, train_dataset, batch_size, epsilon, gamma)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # print the loss and epoch\n",
    "        print(\"epoch: \", epoch, \"loss: \", loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network = Network(21, 128, 3, activation = nn.Softmax())\n",
    "value_network = Network(21, 128, 1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_network.to(device)\n",
    "value_network.to(device)\n",
    "\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=0.001)\n",
    "value_optimizer = optim.Adam(value_network.parameters(), lr=0.001)\n",
    "\n",
    "# convert the training data into tensors\n",
    "train_dataset = torch.tensor(train_data.values, dtype=torch.float32)\n",
    "train_dataset = train_dataset.to(device)\n",
    "\n",
    "# convert the test data into tensors\n",
    "test_dataset = torch.tensor(test_data.values, dtype=torch.float32)\n",
    "test_dataset = test_dataset.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = train_dataset[:150, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\specf\\AppData\\Local\\Temp\\ipykernel_19520\\3401228888.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.activation(self.fc3(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.98779\n",
      "Epoch: 1, Loss: 1.55526\n",
      "Epoch: 2, Loss: 1.20920\n",
      "Epoch: 3, Loss: 1.39280\n",
      "Epoch: 4, Loss: 1.54465\n",
      "Epoch: 5, Loss: 1.76629\n",
      "Epoch: 6, Loss: 1.96971\n",
      "Epoch: 7, Loss: 1.63758\n",
      "Epoch: 8, Loss: 0.79369\n",
      "Epoch: 9, Loss: 0.28855\n",
      "epoch:  0 loss:  0.36139407753944397\n",
      "epoch:  1 loss:  0.36139407753944397\n",
      "Epoch: 0, Loss: 0.64564\n",
      "Epoch: 1, Loss: 1.26349\n",
      "Epoch: 2, Loss: 1.28218\n",
      "Epoch: 3, Loss: 0.75245\n",
      "Epoch: 4, Loss: 0.35962\n",
      "Epoch: 5, Loss: 0.38034\n",
      "Epoch: 6, Loss: 0.52212\n",
      "Epoch: 7, Loss: 0.57039\n",
      "Epoch: 8, Loss: 0.58788\n",
      "Epoch: 9, Loss: 0.56036\n",
      "epoch:  0 loss:  0.47627732157707214\n",
      "epoch:  1 loss:  0.47627732157707214\n",
      "Epoch: 0, Loss: 0.38274\n",
      "Epoch: 1, Loss: 0.18178\n",
      "Epoch: 2, Loss: 0.20872\n",
      "Epoch: 3, Loss: 0.40537\n",
      "Epoch: 4, Loss: 0.46118\n",
      "Epoch: 5, Loss: 0.30234\n",
      "Epoch: 6, Loss: 0.16169\n",
      "Epoch: 7, Loss: 0.17375\n",
      "Epoch: 8, Loss: 0.23205\n",
      "Epoch: 9, Loss: 0.24484\n",
      "epoch:  0 loss:  -0.23284707963466644\n",
      "epoch:  1 loss:  -0.23284707963466644\n",
      "Epoch: 0, Loss: 0.23987\n",
      "Epoch: 1, Loss: 0.22013\n",
      "Epoch: 2, Loss: 0.15785\n",
      "Epoch: 3, Loss: 0.11322\n",
      "Epoch: 4, Loss: 0.15195\n",
      "Epoch: 5, Loss: 0.20651\n",
      "Epoch: 6, Loss: 0.18347\n",
      "Epoch: 7, Loss: 0.12184\n",
      "Epoch: 8, Loss: 0.10685\n",
      "Epoch: 9, Loss: 0.12863\n",
      "epoch:  0 loss:  -0.25518539547920227\n",
      "epoch:  1 loss:  -0.25518539547920227\n",
      "Epoch: 0, Loss: 0.13724\n",
      "Epoch: 1, Loss: 0.13307\n",
      "Epoch: 2, Loss: 0.12658\n",
      "Epoch: 3, Loss: 0.10757\n",
      "Epoch: 4, Loss: 0.09093\n",
      "Epoch: 5, Loss: 0.10231\n",
      "Epoch: 6, Loss: 0.11976\n",
      "Epoch: 7, Loss: 0.10924\n",
      "Epoch: 8, Loss: 0.08754\n",
      "Epoch: 9, Loss: 0.08475\n",
      "epoch:  0 loss:  0.05036643519997597\n",
      "epoch:  1 loss:  0.05036643519997597\n",
      "Epoch: 0, Loss: 0.09226\n",
      "Epoch: 1, Loss: 0.09287\n",
      "Epoch: 2, Loss: 0.09007\n",
      "Epoch: 3, Loss: 0.08587\n",
      "Epoch: 4, Loss: 0.07772\n",
      "Epoch: 5, Loss: 0.07490\n",
      "Epoch: 6, Loss: 0.08110\n",
      "Epoch: 7, Loss: 0.08228\n",
      "Epoch: 8, Loss: 0.07404\n",
      "Epoch: 9, Loss: 0.06898\n",
      "epoch:  0 loss:  -0.15633060038089752\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m new_policy_network\u001b[39m.\u001b[39mload_state_dict(policy_network\u001b[39m.\u001b[39mstate_dict())\n\u001b[0;32m      6\u001b[0m new_policy_optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(new_policy_network\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m ppo_train(new_policy_network, policy_network, value_network, new_policy_optimizer, small_train_dataset, \u001b[39m12\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m0.2\u001b[39;49m, \u001b[39m0.99\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[60], line 28\u001b[0m, in \u001b[0;36mppo_train\u001b[1;34m(new_policy_network, policy_network, value_network, optimizer, train_dataset, batch_size, epochs, epsilon, gamma)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mppo_train\u001b[39m(new_policy_network, policy_network, value_network, optimizer, train_dataset, batch_size, epochs, epsilon\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, gamma \u001b[39m=\u001b[39m \u001b[39m0.99\u001b[39m):\n\u001b[0;32m     27\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> 28\u001b[0m         loss \u001b[39m=\u001b[39m ppo_loss(new_policy_network, policy_network, value_network, train_dataset, batch_size, epsilon, gamma)\n\u001b[0;32m     29\u001b[0m         loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     30\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[60], line 6\u001b[0m, in \u001b[0;36mppo_loss\u001b[1;34m(new_policy_network, policy_network, value_network, train_dataset, batch_size, epsilon, gamma)\u001b[0m\n\u001b[0;32m      4\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[1;32m----> 6\u001b[0m     states, rewards, action_dist_set, action_set \u001b[39m=\u001b[39m get_path(policy_network, train_dataset)\n\u001b[0;32m      7\u001b[0m     new_action_dist_set \u001b[39m=\u001b[39m new_policy_network(states)\n\u001b[0;32m      8\u001b[0m     action_dist_set \u001b[39m=\u001b[39m action_dist_set\u001b[39m.\u001b[39mdetach()\n",
      "Cell \u001b[1;32mIn[43], line 15\u001b[0m, in \u001b[0;36mget_path\u001b[1;34m(policy_network, train_dataset)\u001b[0m\n\u001b[0;32m     13\u001b[0m hold \u001b[39m=\u001b[39m holding_positions[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[39m# concat bid_ask and hold to get the state\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((bid_ask, torch\u001b[39m.\u001b[39;49mtensor([hold])\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39mto(device)), dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m     16\u001b[0m states\u001b[39m.\u001b[39mappend(state)\n\u001b[0;32m     17\u001b[0m \u001b[39m# get the action from the policy network, which is a probability distribution\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iter = 10\n",
    "for iteration in range(num_iter):\n",
    "    train_value_network(value_network, policy_network, small_train_dataset, value_optimizer, 0.9)\n",
    "    new_policy_network = Network(21, 128, 3, activation = nn.Softmax()).to(device)\n",
    "    new_policy_network.load_state_dict(policy_network.state_dict())\n",
    "    new_policy_optimizer = optim.Adam(new_policy_network.parameters(), lr = 0.001)\n",
    "    ppo_train(new_policy_network, policy_network, value_network, new_policy_optimizer, small_train_dataset, 12, 2, 0.2, 0.99)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
