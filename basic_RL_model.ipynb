{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib ipympl\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, output_size, activation = nn.Identity()):\n",
    "        super(Network, self).__init__()\n",
    "        # in this case, the number of channels is 10 because we retrieve last 10 snapshots\n",
    "        self.conv1 = nn.Conv1d(in_channels=10, out_channels=10, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv1d(in_channels=10, out_channels=10, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv1d(in_channels=10, out_channels=10, kernel_size=3, stride=1, padding=0)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(11, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "        self.activation = activation\n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.conv1(x1) \n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.conv3(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = x1.reshape(10, 1, 1)\n",
    "        x1, _ = self.lstm(x1)\n",
    "        x1 = x1.reshape(10, )\n",
    "        x1 = torch.cat((x1, x2), dim = 0)\n",
    "        x1 = self.fc1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.fc2(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.fc3(x1)\n",
    "        x1 = self.activation(x1)\n",
    "        return x1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         AskPrice1  AskVolume1  BidPrice1  BidVolume1  AskPrice2  AskVolume2  \\\n",
      "1887801      769.0          40      768.5         116      769.5         293   \n",
      "1887802      768.5           5      768.0         293      769.0         132   \n",
      "1887803      768.5           4      768.0         303      769.0         311   \n",
      "1887804      769.0         185      768.5          24      769.5         409   \n",
      "1887805      769.0         163      768.5          23      769.5         417   \n",
      "\n",
      "         BidPrice2  BidVolume2  AskPrice3  AskVolume3  BidPrice3  BidVolume3  \n",
      "1887801      768.0         293      770.0         603      767.5         154  \n",
      "1887802      767.5         184      769.5         338      767.0         323  \n",
      "1887803      767.5         305      769.5         396      767.0         364  \n",
      "1887804      768.0         357      770.0         754      767.5         317  \n",
      "1887805      768.0         458      770.0         765      767.5         319  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = df[df['trading_day'] == 25]\n",
    "train_data = train_data.drop(['trading_day'], axis=1)\n",
    "train_data = train_data[['AskPrice1', 'AskVolume1', 'BidPrice1', 'BidVolume1', 'AskPrice2', 'AskVolume2', 'BidPrice2', 'BidVolume2', 'AskPrice3', 'AskVolume3', 'BidPrice3', 'BidVolume3']]\n",
    "train_data = train_data.iloc[:4000, :]\n",
    "print(train_data.head())\n",
    "train_data = train_data.to_numpy()\n",
    "train_data = torch.from_numpy(train_data)\n",
    "train_data = train_data.float()\n",
    "train_data = train_data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, I think it makes sense that to consider snapshots for the last 10 seconds as an input, and start from there. For each snapshot, we only use the first three bid-ask price level, which means 12 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to get a path from the policy network\n",
    "def get_path(policy_network, train_dataset):\n",
    "    holding_positions = [0]\n",
    "    pos_changes = []\n",
    "    rewards = []\n",
    "    states1 = []\n",
    "    states2 = [] \n",
    "    action_dist_set = []\n",
    "    action_set = []\n",
    "    for i in range(9, len(train_dataset)):\n",
    "\n",
    "        # bid_ask information of past 10 snapshots\n",
    "        bid_ask = train_dataset[i - 9:i + 1, :]\n",
    "        bid_ask = bid_ask.unsqueeze(0)\n",
    "        old_hold = holding_positions[-1]\n",
    "        hold = torch.tensor([old_hold]).float().to(device)\n",
    "        states1.append(bid_ask)\n",
    "        states2.append(hold)\n",
    "        # get the action from the policy network, which is a probability distribution\n",
    "        action_dist = policy_network(bid_ask, hold)\n",
    "        action_dist_set.append(action_dist)\n",
    "        # sample an action from the probability distribution\n",
    "        action = torch.multinomial(action_dist, 1).item() - 1\n",
    "        action_set.append(action + 1)\n",
    "        \n",
    "        # decide the position change based on the action and current holding position\n",
    "        if old_hold == 0:\n",
    "            # make the action to be integer\n",
    "            a = int(action)\n",
    "            holding_positions.append(a)\n",
    "        if old_hold == 1:\n",
    "            holding_positions.append(min(action + old_hold, old_hold))\n",
    "        if old_hold == -1:\n",
    "            holding_positions.append(max(action + old_hold, old_hold))\n",
    "           \n",
    "        # compute the reward (cash change) \n",
    "        new_hold = holding_positions[-1]\n",
    "        position_change = new_hold - old_hold\n",
    "        pos_changes.append(position_change)\n",
    "    \n",
    "        reward = 0\n",
    "        if position_change == 0:\n",
    "            reward = 0\n",
    "        if position_change == 1:\n",
    "            reward = - train_dataset[i][2] \n",
    "        if position_change == -1:\n",
    "            reward = train_dataset[i][0] \n",
    "\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    # at the end, the agent needs to liquid all positions\n",
    "    if holding_positions[-1] == 1:\n",
    "        rewards.append(train_dataset[-1][2])\n",
    "    elif holding_positions[-1] == -1:\n",
    "        rewards.append(-train_dataset[-1][0])\n",
    "    else:\n",
    "        rewards.append(0)\n",
    "\n",
    "    # convert the list of rewards to a tensor\n",
    "    rewards = torch.tensor(rewards).float().to(device)\n",
    "    # convert the list of states to a tensor\n",
    "    states1 = torch.stack(states1).to(device)\n",
    "    states2 = torch.stack(states2).to(device)\n",
    "    action_dist_set = torch.stack(action_dist_set).to(device)\n",
    "    return states1, states2, rewards, action_dist_set, action_set\n",
    "\n",
    "\n",
    "        \n",
    "# train the value network using the TD(0) algorithm\n",
    "def train_value_network(value_network, policy_network, train_dataset, value_optimizer, gamma = 0.99, epochs = 10):\n",
    "    # define the loss function\n",
    "    loss = 0\n",
    "    # loop over the epochs\n",
    "    for epoch in range(epochs):\n",
    "        # get the path from the policy network\n",
    "        # only use one trajectory each epoch\n",
    "        states1, states2, rewards, action_dist_set, action_set = get_path(policy_network, train_dataset)\n",
    "        # get the value estimate from the value network\n",
    "        value_estimate = []\n",
    "        for i in range(len(states1)):\n",
    "            value_estimate.append(value_network(states1[i], states2[i]))\n",
    "        value_estimate = torch.stack(value_estimate).squeeze(1)\n",
    "        new_value_estimate = torch.cat((value_estimate, torch.tensor([0]).to(device)))\n",
    "        # compute the TD(0) error\n",
    "        loss = (rewards[:-1] + gamma * new_value_estimate[1:] - new_value_estimate[:-1]).pow(2).mean()\n",
    "        # zero the gradient\n",
    "        value_optimizer.zero_grad()\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        value_optimizer.step()\n",
    "        # print the loss\n",
    "        print('Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))\n",
    "    return None\n",
    "\n",
    "\n",
    "# the following define a function that compute advantage estimation for a trajectory\n",
    "def advantage_estimate(states1, states2, rewards, value_network, gamma):\n",
    "    value_estimate = []\n",
    "    for i in range(len(states1)):\n",
    "        value_estimate.append(value_network(states1[i], states2[i]))\n",
    "    value_estimate = torch.stack(value_estimate).squeeze(1)\n",
    "    new_value_estimate = torch.cat((value_estimate, torch.tensor([0]).to(device)))\n",
    "    advantages = rewards[:-1] + gamma * new_value_estimate[1:] - new_value_estimate[:-1]\n",
    "    return advantages\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states, rewards, actions are trajetory data of old policy\n",
    "# there is a new_policy_network that is updated by ppo_update()\n",
    "def ppo_loss(new_policy_network, policy_network, value_network, train_dataset, batch_size, epsilon=0.2, gamma = 0.99):\n",
    "    # batch size: the number of trajectories\n",
    "    loss = torch.tensor(0.0, requires_grad=True).to(device)\n",
    "    for _ in range(batch_size):\n",
    "        states1, states2, rewards, action_dist_set, action_set = get_path(policy_network, train_dataset)\n",
    "        new_action_dist_set = [policy_network(states1[i], states2[i]) for i in range(len(states1))]\n",
    "        new_action_dist_set = torch.stack(new_action_dist_set).to(device)\n",
    "        action_dist_set = action_dist_set.detach()\n",
    "        rewards = rewards.detach()\n",
    "        ratio = []\n",
    "        for i in range(len(action_dist_set)):\n",
    "            ratio.append(new_action_dist_set[i][action_set[i]] / action_dist_set[i][action_set[i]])\n",
    "        ratio = torch.stack(ratio).to(device).detach()\n",
    "        # compute the advantage of the trajectory\n",
    "        advantage = advantage_estimate(states1, states2, rewards, value_network, gamma)\n",
    "        advantage = advantage.detach()\n",
    "        # compute the clipped ratio\n",
    "        clipped_ratio = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "        # compute the surrogate loss\n",
    "        policy_loss = -torch.min(ratio * advantage, clipped_ratio * advantage).mean()\n",
    "        # compute the total loss\n",
    "        loss = loss + policy_loss\n",
    "    return loss\n",
    "\n",
    "    \n",
    "def ppo_train(new_policy_network, policy_network, value_network, optimizer, train_dataset, batch_size, epochs, epsilon=0.2, gamma = 0.99):\n",
    "    # this function is used to train the new_policy_network\n",
    "    for epoch in range(epochs):\n",
    "        loss = ppo_loss(new_policy_network, policy_network, value_network, train_dataset, batch_size, epsilon, gamma)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # print the loss and epoch\n",
    "        print(\"epoch: \", epoch, \"loss: \", loss.item())\n",
    "\n",
    "\n",
    "def wealth_dist(num_traj, policy_network, dataset):\n",
    "    wealths = []\n",
    "    for num in range(num_traj):\n",
    "        states1, states2, rewards, action_dist_set, action_set = get_path(policy_network, dataset)\n",
    "        wealths.append(sum(rewards))\n",
    "\n",
    "    return wealths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network = Network(3, activation = nn.Softmax())\n",
    "value_network = Network(1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_network.to(device)\n",
    "value_network.to(device)\n",
    "\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=0.1)\n",
    "value_optimizer = optim.Adam(value_network.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\specf\\AppData\\Local\\Temp\\ipykernel_14820\\4000364149.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x1 = self.activation(x1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 272686.90625\n",
      "Epoch: 1, Loss: 264782.40625\n",
      "Epoch: 2, Loss: 268063.59375\n",
      "Epoch: 3, Loss: 265880.40625\n",
      "Epoch: 4, Loss: 270217.68750\n",
      "Epoch: 5, Loss: 268513.34375\n",
      "Epoch: 6, Loss: 261726.56250\n",
      "Epoch: 7, Loss: 253950.43750\n",
      "Epoch: 8, Loss: 264373.21875\n",
      "Epoch: 9, Loss: 253227.62500\n",
      "epoch:  0 loss:  -0.10315828025341034\n",
      "epoch:  1 loss:  -0.2628318667411804\n",
      "epoch:  2 loss:  -0.5058820247650146\n",
      "epoch:  3 loss:  -0.14138999581336975\n",
      "epoch:  4 loss:  -0.09657317399978638\n",
      "epoch:  5 loss:  0.07471542805433273\n",
      "epoch:  6 loss:  0.06681688129901886\n",
      "epoch:  7 loss:  -0.702826976776123\n",
      "epoch:  8 loss:  -0.29848170280456543\n",
      "epoch:  9 loss:  -0.6828583478927612\n",
      "epoch:  10 loss:  -0.10874849557876587\n",
      "epoch:  11 loss:  -0.27514711022377014\n",
      "epoch:  12 loss:  -0.13941770792007446\n",
      "epoch:  13 loss:  -0.46001237630844116\n",
      "epoch:  14 loss:  -0.0982646644115448\n",
      "Epoch: 0, Loss: 242559.14062\n",
      "Epoch: 1, Loss: 238693.00000\n",
      "Epoch: 2, Loss: 231260.04688\n",
      "Epoch: 3, Loss: 233034.73438\n",
      "Epoch: 4, Loss: 227410.82812\n",
      "Epoch: 5, Loss: 222005.23438\n",
      "Epoch: 6, Loss: 220172.43750\n",
      "Epoch: 7, Loss: 211999.70312\n",
      "Epoch: 8, Loss: 206002.26562\n",
      "Epoch: 9, Loss: 202812.79688\n",
      "epoch:  0 loss:  -0.3888660669326782\n",
      "epoch:  1 loss:  -0.279670774936676\n",
      "epoch:  2 loss:  0.41823458671569824\n",
      "epoch:  3 loss:  -0.1474504917860031\n",
      "epoch:  4 loss:  -0.16853952407836914\n",
      "epoch:  5 loss:  -0.37625133991241455\n",
      "epoch:  6 loss:  0.10667538642883301\n",
      "epoch:  7 loss:  0.07788044214248657\n",
      "epoch:  8 loss:  0.0513298362493515\n",
      "epoch:  9 loss:  0.7198479175567627\n",
      "epoch:  10 loss:  0.661877453327179\n",
      "epoch:  11 loss:  0.5431708693504333\n",
      "epoch:  12 loss:  -0.2528229057788849\n",
      "epoch:  13 loss:  0.22117513418197632\n",
      "epoch:  14 loss:  -0.5825290083885193\n",
      "Epoch: 0, Loss: 198203.28125\n",
      "Epoch: 1, Loss: 190502.98438\n",
      "Epoch: 2, Loss: 175982.34375\n",
      "Epoch: 3, Loss: 174143.28125\n",
      "Epoch: 4, Loss: 167639.18750\n",
      "Epoch: 5, Loss: 156179.54688\n",
      "Epoch: 6, Loss: 147055.07812\n",
      "Epoch: 7, Loss: 137082.81250\n",
      "Epoch: 8, Loss: 122959.91406\n",
      "Epoch: 9, Loss: 116078.62500\n",
      "epoch:  0 loss:  -0.9465162754058838\n",
      "epoch:  1 loss:  -1.070793867111206\n",
      "epoch:  2 loss:  -0.8319960832595825\n",
      "epoch:  3 loss:  -0.6167178750038147\n",
      "epoch:  4 loss:  -0.7079728841781616\n",
      "epoch:  5 loss:  0.00295373797416687\n",
      "epoch:  6 loss:  -0.6218603253364563\n",
      "epoch:  7 loss:  -0.5838501453399658\n",
      "epoch:  8 loss:  -0.8226631879806519\n",
      "epoch:  9 loss:  -1.3773994445800781\n",
      "epoch:  10 loss:  -1.2328596115112305\n",
      "epoch:  11 loss:  -1.070938229560852\n",
      "epoch:  12 loss:  -0.6443721055984497\n",
      "epoch:  13 loss:  -0.3897993862628937\n",
      "epoch:  14 loss:  -1.3132588863372803\n",
      "Epoch: 0, Loss: 107452.01562\n",
      "Epoch: 1, Loss: 97782.71094\n",
      "Epoch: 2, Loss: 85986.59375\n",
      "Epoch: 3, Loss: 74235.53125\n",
      "Epoch: 4, Loss: 65323.30859\n",
      "Epoch: 5, Loss: 52968.81641\n",
      "Epoch: 6, Loss: 44605.57422\n",
      "Epoch: 7, Loss: 35034.43750\n",
      "Epoch: 8, Loss: 26980.55078\n",
      "Epoch: 9, Loss: 18952.45117\n",
      "epoch:  0 loss:  0.5850159525871277\n",
      "epoch:  1 loss:  0.2463940531015396\n",
      "epoch:  2 loss:  -0.34932661056518555\n",
      "epoch:  3 loss:  -0.18280547857284546\n",
      "epoch:  4 loss:  -1.3893855810165405\n",
      "epoch:  5 loss:  -1.5333878993988037\n",
      "epoch:  6 loss:  -0.743624210357666\n",
      "epoch:  7 loss:  0.2170754373073578\n",
      "epoch:  8 loss:  0.17326655983924866\n",
      "epoch:  9 loss:  0.17847245931625366\n",
      "epoch:  10 loss:  -0.16912177205085754\n",
      "epoch:  11 loss:  -0.4147742688655853\n",
      "epoch:  12 loss:  0.4813234508037567\n",
      "epoch:  13 loss:  -0.07572665810585022\n",
      "epoch:  14 loss:  -0.5897072553634644\n",
      "Epoch: 0, Loss: 12701.80664\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m# num_iter this is the number of times that we improve the policy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iter):\n\u001b[1;32m----> 4\u001b[0m     train_value_network(value_network, policy_network, train_data, value_optimizer, \u001b[39m0.9\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m     new_policy_network \u001b[39m=\u001b[39m Network(\u001b[39m3\u001b[39m, activation \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax())\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m     new_policy_optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(new_policy_network\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 89\u001b[0m, in \u001b[0;36mtrain_value_network\u001b[1;34m(value_network, policy_network, train_dataset, value_optimizer, gamma, epochs)\u001b[0m\n\u001b[0;32m     87\u001b[0m value_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     88\u001b[0m \u001b[39m# compute the gradient\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     90\u001b[0m \u001b[39m# update the weights\u001b[39;00m\n\u001b[0;32m     91\u001b[0m value_optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\specf\\anaconda3\\envs\\qids-2023-comp\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\specf\\anaconda3\\envs\\qids-2023-comp\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iter = 5\n",
    "# num_iter this is the number of times that we improve the policy\n",
    "for iteration in range(num_iter):\n",
    "    train_value_network(value_network, policy_network, train_data, value_optimizer, 0.9)\n",
    "    new_policy_network = Network(3, activation = nn.Softmax()).to(device)\n",
    "    new_policy_optimizer = optim.Adam(new_policy_network.parameters(), lr = 0.1)\n",
    "    ppo_train(new_policy_network, policy_network, value_network, new_policy_optimizer, train_data, batch_size = 5, epochs = 15, epsilon = 0.2, gamma = 0.99)\n",
    "    policy_network = new_policy_network\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\specf\\AppData\\Local\\Temp\\ipykernel_14820\\4000364149.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x1 = self.activation(x1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(445., device='cuda:0'), tensor(434., device='cuda:0'), tensor(441.5000, device='cuda:0'), tensor(433.5000, device='cuda:0'), tensor(444.5000, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "w = wealth_dist(5, policy_network, train_data)\n",
    "print(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
